{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0418909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import copy\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f59270",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c051694a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available and use it, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59389f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def remove_extra_images(root_folder, max_images_per_folder=1000, random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    for folder in os.listdir(root_folder):\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            images = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "            random.shuffle(images)\n",
    "            \n",
    "            for image in images[max_images_per_folder:]:\n",
    "                image_path = os.path.join(folder_path, image)\n",
    "                os.remove(image_path)\n",
    "\n",
    "root_folder = '../data/art-styles'\n",
    "remove_extra_images(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4c757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1195f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfabfd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(root='../data/art-styles', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b9705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9def3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# # Loop through all images in the dataset\n",
    "# for i in range(len(dataset)):\n",
    "#     image_path, _ = dataset.samples[i]\n",
    "#     try:\n",
    "#         img = Image.open(image_path)\n",
    "#         img.verify() # This will raise an exception if the file is corrupted\n",
    "#         if not any(_ in image_path for _ in ['Impressionism',\n",
    "#  'Realism',\n",
    "#  'Romanticism',\n",
    "#  'Expressionism',\n",
    "#  'Post_Impressionism',\n",
    "#  'Symbolism',\n",
    "#  'Art_Nouveau_Modern',\n",
    "#  'Baroque',\n",
    "#  'Abstract_Expressionism',\n",
    "#  'Northern_Renaissance']):\n",
    "#             os.remove(image_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Image {image_path} is corrupted: {e}\")\n",
    "# #         os.remove(image_path) # Delete the file if it's corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef28b9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Art_Nouveau_Modern', 1000),\n",
       " ('Baroque', 1000),\n",
       " ('Impressionism', 1000),\n",
       " ('Northern_Renaissance', 1000),\n",
       " ('Post_Impressionism', 1000),\n",
       " ('Realism', 1000),\n",
       " ('Romanticism', 1000),\n",
       " ('Symbolism', 1000),\n",
       " ('Abstract_Expressionism', 999),\n",
       " ('Expressionism', 999)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([dataset.classes[t] for t in dataset.targets]).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271b129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87521e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# shutil.rmtree(f'../data/art-styles/{directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bd8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35814a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7998, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca748d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fc290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01261a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_size=None):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb37912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x28dddfa30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a866bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76464119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a03166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, *extra in tqdm(test_loader, total=len(test_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            for e in extra:\n",
    "                e = e.to(device)   \n",
    "            outputs = model(inputs, *extra)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "    \n",
    "    accuracy, f1, precision, recall = evaluate_model_metrics(np.array(y_true), np.array(y_pred))\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\\n')\n",
    "\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluate_model_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return accuracy, f1, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd6b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    test_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            dataloader = tqdm(dataloaders[phase], total=len(dataloaders[phase]))\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels, *extra in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                for e in extra:\n",
    "                    e = e.to(device)    \n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs, *extra)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            # Deep copy the model\n",
    "            if phase == \"test\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == \"test\":\n",
    "                test_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Best test Acc: {best_acc:4f}\")\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, test_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b2b8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24bc1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dataset.classes)  # Number of classes in your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a26a2f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100352"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample input tensor with the size of (batch_size, channels, height, width)\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "model = CNN(num_classes=num_classes, input_size=1)\n",
    "# Perform a forward pass through the convolutional layers\n",
    "sample_output = model.conv1(sample_input)\n",
    "sample_output = model.pool(sample_output)\n",
    "sample_output = model.conv2(sample_output)\n",
    "sample_output = model.pool(sample_output)\n",
    "\n",
    "# Calculate the size of the resulting tensor\n",
    "input_size = sample_output.numel()\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fccc97f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abstract_Expressionism',\n",
       " 'Art_Nouveau_Modern',\n",
       " 'Baroque',\n",
       " 'Expressionism',\n",
       " 'Impressionism',\n",
       " 'Northern_Renaissance',\n",
       " 'Post_Impressionism',\n",
       " 'Realism',\n",
       " 'Romanticism',\n",
       " 'Symbolism']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e271328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create an instance of the CNN model\n",
    "cnn_model = CNN(num_classes=num_classes, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d813c7d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:36<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1015, F1-score: 0.0560, Precision: 0.1606, Recall: 0.1031\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c2976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e637e7fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:03<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.6537 Acc: 0.1715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:34<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.1194 Acc: 0.2180\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.9916 Acc: 0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:35<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0504 Acc: 0.2545\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5859 Acc: 0.4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:35<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.0602 Acc: 0.2825\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8457 Acc: 0.7393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:34<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 2.4143 Acc: 0.2740\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [02:02<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2900 Acc: 0.9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:35<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 3.1558 Acc: 0.2660\n",
      "\n",
      "Training complete in 13m 8s\n",
      "Best test Acc: 0.282500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a dictionary containing both training and validation DataLoaders\n",
    "dataloaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "\n",
    "# Step 3: Define a loss function and an optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 4: Train the model by calling the train_model function\n",
    "num_epochs = 5\n",
    "trained_cnn_model, test_acc_history = train_model(cnn_model, dataloaders, criterion, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9558755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:34<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2825, F1-score: 0.2771, Precision: 0.3041, Recall: 0.2840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(trained_cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78c99361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(trained_cnn_model.state_dict(), \"../data/trained_cnn_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a0a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bddabdae",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cc066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeacc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41919ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ae6f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "csv_path = '../data/disaster.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Tokenize the text column\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenized = [tokenizer(text) for text in df['text']]\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized)\n",
    "\n",
    "# Encode the target column\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['target'])\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        tokens = tokenizer(text)\n",
    "        encoded_text = [vocab[token] for token in tokens]\n",
    "        target = self.data.iloc[idx]['target']\n",
    "        return torch.tensor(encoded_text), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1213f7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.57034\n",
       "1    0.42966\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d55f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the dataloaders\n",
    "train_dataset = TextDataset(train_data, vocab)\n",
    "test_dataset = TextDataset(test_data, vocab)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (sequences, targets) = zip(*batch)\n",
    "    lengths = torch.tensor([len(sequence) for sequence in sequences])\n",
    "    sequences = pad_sequence(sequences, batch_first=True)\n",
    "    targets = torch.tensor(targets)\n",
    "    return sequences, targets, lengths\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, collate_fn=pad_collate)\n",
    "\n",
    "# Step 2: Create the LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        x = self.fc(hidden.squeeze(0))\n",
    "        return x\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "lstm = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e964f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO see if we can use the old train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20cbc852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 482.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5161, F1-score: 0.5159, Precision: 0.5341, Recall: 0.5335\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(lstm, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "732c197b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 762/762 [00:08<00:00, 93.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.6233 Acc: 0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 498.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.5846 Acc: 0.7137\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 762/762 [00:08<00:00, 94.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.5182 Acc: 0.7552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 500.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.5597 Acc: 0.7393\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 762/762 [00:08<00:00, 92.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3888 Acc: 0.8361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 493.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.6149 Acc: 0.7177\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 762/762 [00:08<00:00, 92.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2878 Acc: 0.8836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 490.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.6766 Acc: 0.7157\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 762/762 [00:08<00:00, 91.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1899 Acc: 0.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 492.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.9144 Acc: 0.6953\n",
      "\n",
      "Training complete in 0m 43s\n",
      "Best test Acc: 0.739330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a dictionary containing both training and validation DataLoaders\n",
    "dataloaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "\n",
    "# Step 4: Train the model by calling the train_model function\n",
    "num_epochs = 5\n",
    "trained_lstm_model, test_acc_history = train_model(lstm, dataloaders, criterion, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d62c125f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 191/191 [00:00<00:00, 494.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7393, F1-score: 0.7326, Precision: 0.7323, Recall: 0.7328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(trained_lstm_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f8fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b921203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c26ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
