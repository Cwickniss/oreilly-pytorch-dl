{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bdfba6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "deb4a334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112c28e10>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize random numbers for reproducibility\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f54d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer, a single hidden layer, and an output layer\n",
    "# Define hyperparameters\n",
    "input_size = 784  # 28x28 pixels\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf93c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.datasets.MNIST(root='../data', train=True, download=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0d7a5-81ec-46fb-ae90-e9084b6e2c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0de01bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),   # convert to pytorch tensor\n",
    "     transforms.Normalize((0.1307,), (0.3081,))  # standardize the values with mean + std. Found before running code\n",
    "    ]  \n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fc713e63-5149-4501-b718-1c31a8adeff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0227)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e926a88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transpose the array to (28, 28) format expected by Matplotlib\n",
    "array = np.squeeze(np.transpose(train_dataset[0][0], (1, 2, 0)))\n",
    "\n",
    "# plot the image using Matplotlib\n",
    "plt.imshow(array)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42f64815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9ef55b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Create the model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fca379b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some loss function options\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "nll_loss = nn.NLLLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8df8e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "502faaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, loss_function, train_loader, optimizer, num_epochs=10):\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the running loss for this epoch to zero\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Iterate over each batch in the training loader\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # Reshape the images tensor to have size (batch_size, input_size)\n",
    "            images = images.reshape(-1, input_size)\n",
    "            # Forward pass: compute the outputs of the model given the input images\n",
    "            outputs = model(images)\n",
    "            # Compute the loss between the outputs and the true labels\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # Backward pass: compute the gradients of the loss with respect to the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Update the model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the current batch loss to the running loss for this epoch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute the average loss over all batches for this epoch and print it\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "410c88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize empty lists to store true and predicted labels\n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    # Disable gradient computation since we're only evaluating the model\n",
    "    with torch.no_grad():\n",
    "        # Iterate over each batch in the test loader\n",
    "        for images, labels in test_loader:\n",
    "            # Reshape the images tensor to have size (batch_size, input_size)\n",
    "            images = images.reshape(-1, input_size)\n",
    "            \n",
    "            # Forward pass: compute the outputs of the model given the input images\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Find the predicted class for each image in the batch\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Append the true and predicted labels for this batch to the lists\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "    \n",
    "    # Calculate evaluation metrics using the true and predicted labels\n",
    "    accuracy, f1, precision, recall = evaluate_model_metrics(np.array(y_true), np.array(y_pred))\n",
    "    \n",
    "    # Return the evaluation metrics\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluate_model_metrics(y_true, y_pred):\n",
    "    # Compute the accuracy, F1 score, precision, and recall\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Return the evaluation metrics\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Define a dictionary of loss functions\n",
    "loss_functions = {\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "    'NLLLoss': nn.NLLLoss(),  # requires logsoftmax output (according to docs, so loss will be wrong without it!)\n",
    "    'MultiMarginLoss': nn.MultiMarginLoss(),\n",
    "    'KLDivLoss': nn.KLDivLoss()   # requires logsoftmax output (according to docs, so loss will be wrong without it!)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "156ac9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with CrossEntropyLoss:\n",
      "Epoch [1/5], Loss: 0.2866\n",
      "Epoch [2/5], Loss: 0.1269\n",
      "Epoch [3/5], Loss: 0.0869\n",
      "Epoch [4/5], Loss: 0.0659\n",
      "Epoch [5/5], Loss: 0.0511\n",
      "Performance metrics for CrossEntropyLoss:\n",
      "Accuracy: 0.9754, F1-score: 0.9753, Precision: 0.9756, Recall: 0.9752\n",
      "\n",
      "Training with NLLLoss:\n",
      "Epoch [1/5], Loss: -11701.7066\n",
      "Epoch [2/5], Loss: -83472.9563\n",
      "Epoch [3/5], Loss: -217246.9347\n",
      "Epoch [4/5], Loss: -402067.4273\n",
      "Epoch [5/5], Loss: -631296.8376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for NLLLoss:\n",
      "Accuracy: 0.1135, F1-score: 0.0204, Precision: 0.0114, Recall: 0.1000\n",
      "\n",
      "Training with MultiMarginLoss:\n",
      "Epoch [1/5], Loss: 0.0423\n",
      "Epoch [2/5], Loss: 0.0156\n",
      "Epoch [3/5], Loss: 0.0104\n",
      "Epoch [4/5], Loss: 0.0079\n",
      "Epoch [5/5], Loss: 0.0064\n",
      "Performance metrics for MultiMarginLoss:\n",
      "Accuracy: 0.9716, F1-score: 0.9714, Precision: 0.9722, Recall: 0.9710\n",
      "\n",
      "Training with KLDivLoss:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "kl_div: Integral inputs not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m accuracy, f1, precision, recall \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader)\n",
      "Cell \u001b[0;32mIn[118], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_function, train_loader, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute the loss between the outputs and the true labels\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Backward pass: compute the gradients of the loss with respect to the model parameters\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/loss.py:471\u001b[0m, in \u001b[0;36mKLDivLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:2931\u001b[0m, in \u001b[0;36mkl_div\u001b[0;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[1;32m   2928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2929\u001b[0m         reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m-> 2931\u001b[0m reduced \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchmean\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2934\u001b[0m     reduced \u001b[38;5;241m=\u001b[39m reduced \u001b[38;5;241m/\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: kl_div: Integral inputs not supported."
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model using different loss functions\n",
    "for loss_name, loss_function in loss_functions.items():\n",
    "    print(f'Training with {loss_name}:')\n",
    "    \n",
    "    # Initialize a new model and optimizer for each loss function\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, loss_function, train_loader, optimizer, num_epochs=5)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, f1, precision, recall = evaluate_model(model, test_loader)\n",
    "    print(f'Performance metrics for {loss_name}:')\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116ef96-efc8-4699-8dd2-2c3a36b96bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c714633a-9df3-41d9-9c82-6163d5dfb174",
   "metadata": {},
   "source": [
    "# Making our code with with KL and NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdc826-eb71-4974-8bb5-52bbad2784c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cbb6dd60-1a9d-4063-b38e-e82135a335b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, loss_function, train_loader, optimizer, num_epochs=10):\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the running loss for this epoch to zero\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Iterate over each batch in the training loader\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # Reshape the images tensor to have size (batch_size, input_size)\n",
    "            images = images.reshape(-1, input_size)\n",
    "            # Forward pass: compute the outputs of the model given the input images\n",
    "            outputs = model(images)\n",
    "\n",
    "            #### NEW ####\n",
    "            if 'KL' in str(loss_function) or 'NLL' in str(loss_function):\n",
    "                outputs = F.log_softmax(outputs, dim=1)\n",
    "            if 'KL' in str(loss_function):  # KL also need one hot \n",
    "                labels = one_hot_encode(labels, 10)\n",
    "            #### NEW ####\n",
    "            \n",
    "            # Compute the loss between the outputs and the true labels\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # Backward pass: compute the gradients of the loss with respect to the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Update the model parameters using the optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the current batch loss to the running loss for this epoch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Compute the average loss over all batches for this epoch and print it\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cdfab-6a5c-4489-bb54-2fcbd745e1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cacd3c-8dd8-4ce5-a890-9cc35976e6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "49208990-0eb6-493b-91db-7c91196eaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    # Create a tensor of zeros with shape [len(labels), num_classes]\n",
    "    one_hot = torch.zeros(len(labels), num_classes)\n",
    "    \n",
    "    # Use scatter_ to assign 1s to the correct class indices\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    \n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad6603-d429-49b3-9228-db78581bb2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e5c84301-45e4-497d-849b-56124f51f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with NLLLoss:\n",
      "Epoch [1/5], Loss: 0.2944\n",
      "Epoch [2/5], Loss: 0.1269\n",
      "Epoch [3/5], Loss: 0.0859\n",
      "Epoch [4/5], Loss: 0.0655\n",
      "Epoch [5/5], Loss: 0.0511\n",
      "Performance metrics for NLLLoss:\n",
      "Accuracy: 0.9755, F1-score: 0.9754, Precision: 0.9755, Recall: 0.9753\n",
      "\n",
      "Training with KLDivLoss:\n",
      "Epoch [1/5], Loss: 0.2850\n",
      "Epoch [2/5], Loss: 0.1252\n",
      "Epoch [3/5], Loss: 0.0850\n",
      "Epoch [4/5], Loss: 0.0645\n",
      "Epoch [5/5], Loss: 0.0508\n",
      "Performance metrics for KLDivLoss:\n",
      "Accuracy: 0.9765, F1-score: 0.9763, Precision: 0.9762, Recall: 0.9764\n",
      "\n",
      "Training with CrossEntropyLoss:\n",
      "Epoch [1/5], Loss: 0.2846\n",
      "Epoch [2/5], Loss: 0.1241\n",
      "Epoch [3/5], Loss: 0.0854\n",
      "Epoch [4/5], Loss: 0.0650\n",
      "Epoch [5/5], Loss: 0.0518\n",
      "Performance metrics for CrossEntropyLoss:\n",
      "Accuracy: 0.9772, F1-score: 0.9770, Precision: 0.9770, Recall: 0.9770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a dictionary of loss functions\n",
    "loss_functions = {\n",
    "    'NLLLoss': nn.NLLLoss(),  \n",
    "    'KLDivLoss': nn.KLDivLoss(reduction='batchmean'),  # to avoid deprecation warning, I'm setting reduction to batchmean\n",
    "    'CrossEntropyLoss': nn.CrossEntropyLoss()\n",
    "}\n",
    "# Train and evaluate the model using different loss functions\n",
    "for loss_name, loss_function in loss_functions.items():\n",
    "    print(f'Training with {loss_name}:')\n",
    "    \n",
    "    # Initialize a new model and optimizer for each loss function\n",
    "    model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, loss_function, train_loader, optimizer, num_epochs=5)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, f1, precision, recall = evaluate_model(model, test_loader)\n",
    "    print(f'Performance metrics for {loss_name}:')\n",
    "    print(f'Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ff44b62b-0e6c-4a12-8e65-22ca97c1929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# KL is better for imbalanced data, which this isn't so much\n",
    "pd.Series(test_dataset.train_labels).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bad6b3-bb53-4cf2-a3c4-a1431a7c46c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
